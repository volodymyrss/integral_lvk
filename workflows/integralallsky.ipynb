{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:failed to get auth by any method: will not use auth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed to get AUTH with env due to: 'K8S_SECRET_INTEGRAL_CLIENT_SECRET'\n",
      "failed to get AUTH with homefile due to: [Errno 2] No such file or directory: '/home/savchenk/.secret-client-user'\n"
     ]
    }
   ],
   "source": [
    "# %matplotlib notebook\n",
    "\n",
    "from matplotlib import pylab\n",
    "from astropy.coordinates import SkyCoord\n",
    "import integralclient as ic\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from astropy.table import Table, vstack\n",
    "from astropy.io import fits\n",
    "import glob\n",
    "from scipy import stats\n",
    "from astropy.time import Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "RA=293.732\n",
    "Dec=21.8967222\n",
    "tstart_rel_mseconds=300.0\n",
    "tstop_rel_seconds=300.0\n",
    "t0_utc=\"2023-05-22T09:38:05.000000\" \n",
    "# t0_utc=Time(Time(\"2022-10-14T19:21:47\").mjd - 8.632259375000002/24/3600, format='mjd').isot.replace(\" \", \"T\") # hard x-ray\n",
    "# t0_utc=\"2022-10-14T19:21:47\"\n",
    "# rt=1\n",
    "# nrt=1\n",
    "# arc=0\n",
    "required_completeness=0.6\n",
    "# mode=\"rt\" # scw|rt|arc\n",
    "mode=\"scw\" # scw|rt|arc\n",
    "global_snr_threshold=3.\n",
    "negative_excesses=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023-05-22T09:38:05.000000'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0_utc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == \"scw\":\n",
    "    rt=0\n",
    "    nrt=1\n",
    "    arc=0\n",
    "elif mode == \"rt\":\n",
    "    rt=1\n",
    "    nrt=0\n",
    "    arc=0\n",
    "elif mode == \"arc\":\n",
    "    rt=0\n",
    "    nrt=0\n",
    "    arc=1\n",
    "elif mode == \"flags\":\n",
    "    print(\"mode set by flags\")\n",
    "else:\n",
    "    raise Exception(\"unknown mode: {}, allowed: scw, rt\".format(mode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_coord = SkyCoord(RA, Dec, unit = \"deg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /home/savchenk/work/transients/workflows/integral-all-sky\n",
    "\n",
    "import integralenv\n",
    "importlib.reload(integralenv)\n",
    "\n",
    "arc_root_prefix = integralenv.get_arc_root_prefix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'IJD': '8542.402247500000',\n",
       " 'OBT': '9131856381010603',\n",
       " 'OBTFITS': '    2   118882299   713387 :    32 29023 65466 58027',\n",
       " 'REVNUM': '2642',\n",
       " 'SCWID': '264200520021 is close',\n",
       " 'UTC': '2023-05-22T09:38:05.000000',\n",
       " 'YYYYDDDHH': '202314209'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ic.converttime(\"UTC\", t0_utc, \"ANY\")1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8542.605708148149, 8542.4022475, 8542.398775277778, 8542.405719722223)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "now_ijd = float(ic.converttime(\"UTC\",time.strftime(\"%Y-%m-%dT%H:%M:%S\"),\"IJD\"))\n",
    "t0_ijd =  float(ic.converttime(\"UTC\",t0_utc,\"IJD\"))\n",
    "\n",
    "tstart_ijd = t0_ijd - tstart_rel_mseconds/24./3600\n",
    "tstop_ijd = t0_ijd + tstop_rel_seconds/24./3600\n",
    "\n",
    "now_ijd, t0_ijd, tstart_ijd, tstop_ijd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lcs={}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.astro.unige.ch/cdci/astrooda/dispatch-data/gw/integralhk/api/v1.0/genlc/ACS/2023-05-22T09:38:05.000000/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/savchenk/.local/lib/python3.10/site-packages/isdcclient.py:65: ConversionWarning: Some errors were detected !\n",
      "    Line #2 (got 11 columns instead of 13)\n",
      "    Line #3 (got 11 columns instead of 13)\n",
      "    Line #4 (got 14 columns instead of 13)\n",
      "    Line #5 (got 11 columns instead of 13)\n",
      "    Line #6 (got 11 columns instead of 13)\n",
      "  return np.genfromtxt(BytesIO(lc_raw.replace(b\"\\\\n\", b\"\").replace(b\"<br>\", b\"\")), invalid_raise=False)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [10], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m IC \u001b[39m=\u001b[39m isdcclient\u001b[39m.\u001b[39mISDCClient()\n\u001b[1;32m      7\u001b[0m lcs[\u001b[39m'\u001b[39m\u001b[39mACS\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m IC\u001b[39m.\u001b[39mgenlc(\u001b[39m\"\u001b[39m\u001b[39mACS\u001b[39m\u001b[39m\"\u001b[39m, t0_utc, \u001b[39m\"\u001b[39m\u001b[39m%.10lg\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m%\u001b[39m\u001b[39mmax\u001b[39m(tstart_rel_mseconds,tstop_rel_seconds),\u001b[39mformat\u001b[39m\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnumpy\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m lcs[\u001b[39m'\u001b[39;49m\u001b[39mACS\u001b[39;49m\u001b[39m'\u001b[39;49m][:,\u001b[39m1\u001b[39;49m] \u001b[39m=\u001b[39m \u001b[39m0.05\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mgot ACS\u001b[39m\u001b[39m\"\u001b[39m, lcs[\u001b[39m'\u001b[39m\u001b[39mACS\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     11\u001b[0m \u001b[39m#lcs['IBIS/Veto'] = IC.genlc(\"IBIS_VETO\", t0_utc, \"%.10lg\"%max(tstart_rel_mseconds,tstop_rel_seconds),format='numpy')\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[39m#lcs['IBIS/Veto'][:,1] = 8.\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    }
   ],
   "source": [
    "if nrt == 1:\n",
    "    import isdcclient\n",
    "\n",
    "    IC = isdcclient.ISDCClient()\n",
    "\n",
    "    \n",
    "    lcs['ACS'] = IC.genlc(\"ACS\", t0_utc, \"%.10lg\"%max(tstart_rel_mseconds,tstop_rel_seconds),format='numpy')\n",
    "    lcs['ACS'][:,1] = 0.05\n",
    "    print(\"got ACS\", lcs['ACS'])\n",
    "    \n",
    "    #lcs['IBIS/Veto'] = IC.genlc(\"IBIS_VETO\", t0_utc, \"%.10lg\"%max(tstart_rel_mseconds,tstop_rel_seconds),format='numpy')\n",
    "    #lcs['IBIS/Veto'][:,1] = 8.\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "if rt == 1:\n",
    "    r = requests.get(f\"https://www.astro.unige.ch/mmoda/dispatch-data/gw/integralhk/api/v1.0/rtlc/{t0_utc}/{tstop_rel_seconds}?json&prophecy\")\n",
    "    lc = np.array(r.json()['data'])\n",
    "    lcs['ACS'] = np.vstack([lc[:,1], lc[:,1] * 0 + 0.05, lc[:,0]]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pylab as plt\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.step(\n",
    "    (lcs['ACS'][:,0] - t0_ijd)*24*3600,\n",
    "    lcs['ACS'][:,2],\n",
    ")\n",
    "\n",
    "plt.xlim(-5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebin(lc,n,av = False):\n",
    "    if n == 0: return lc\n",
    "    \n",
    "    N=int(lc.shape[0]/n)*n\n",
    "    if av:\n",
    "        return lc[:N].reshape((int(lc.shape[0]/n), n)).mean(1)\n",
    "    else:\n",
    "        return lc[:N].reshape((int(lc.shape[0]/n), n)).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "\n",
    "# if rt == 1:\n",
    "\n",
    "#     got_data = False\n",
    "    \n",
    "#     while not got_data:\n",
    "#         current_rev=float(ic.converttime(\"UTC\",t0_utc,\"REVNUM\"))\n",
    "\n",
    "#         print(\"current rev\", current_rev)\n",
    "\n",
    "#         rtdata_roots=[\n",
    "#             '/unsaved/astro/savchenk/dockers/realtimeacs/docker-ibas/spiacs-lcdump',\n",
    "#             '/rtdata',\n",
    "#             '/mnt/sshfs/isdc-in01//unsaved/astro/savchenk/dockers/realtimeacs/docker-ibas/spiacs-lcdump',    \n",
    "#         ]\n",
    "\n",
    "#         for realtime_dump_root in rtdata_roots + [ None ]:\n",
    "#             #print(\"probing\",realtime_dump_root,\"with\",glob.glob(realtime_dump_root+\"/lcdump-revol-*.csv\"))\n",
    "#             if realtime_dump_root and len(glob.glob(realtime_dump_root+\"/lcdump-revol-*.csv\"))>0:\n",
    "#                 print(\"this\",realtime_dump_root)\n",
    "#                 break\n",
    "\n",
    "#         if not realtime_dump_root:\n",
    "#             raise Exception(\"no realtime archvie found\")\n",
    "\n",
    "#         for rt_fn in reversed(sorted([l for l in glob.glob(realtime_dump_root+\"/lcdump-revol-*.csv\") if \n",
    "#                        float(re.search(\"lcdump-revol-(\\d{4}).*.csv\",l).groups()[0])<=current_rev+1])):\n",
    "\n",
    "#             print(rt_fn)\n",
    "\n",
    "#             rt_lc = np.genfromtxt(rt_fn)\n",
    "\n",
    "#             lcs['ACS']=rt_lc[:,(3,0,2,0)]\n",
    "#             lcs['ACS'][:,1] = 0.05\n",
    "\n",
    "#             first_data = lcs['ACS'][:,0][0]\n",
    "#             last_data = lcs['ACS'][:,0][-1]\n",
    "\n",
    "#             print(\"now\", now_ijd, \n",
    "#                   \"first data in file\", first_data, \n",
    "#                   \"last data\", last_data, \n",
    "#                   \"requested\", t0_ijd, \n",
    "#                   \"have margin\", (last_data-t0_ijd)*24*3600,\"s\",\n",
    "#                   \"data delay\", (now_ijd-last_data)*24*3600,\"s\")       \n",
    "\n",
    "\n",
    "#             if t0_ijd<first_data:\n",
    "#                 print(\"data in the previous file\")\n",
    "#                 continue\n",
    "                \n",
    "\n",
    "#             print(\"margin\",(last_data-now_ijd)*24*3600-tstop_rel_seconds*1.5 + 100)\n",
    "#             if  (last_data-t0_ijd)*24*3600>tstop_rel_seconds*1.5 + 100:                            \n",
    "#                 print(\"this margin is sufficient\")\n",
    "#                 got_data=True\n",
    "#                 break\n",
    "#             else:\n",
    "#                 print(\"this margin is NOT sufficient, waiting\")\n",
    "#             #    if (now_ijd-last_data)*24*3600>1000:\n",
    "#             #        raise RuntimeError('margin insufficent, data too old: no more hope')                \n",
    "\n",
    "#                 time.sleep(30)\n",
    "#                 break\n",
    "        \n",
    "            \n",
    "            \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lcs['ACS']\n",
    "\n",
    "lcs['ACS'][:,0].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = dict()    \n",
    "\n",
    "for n, lc in lcs.items():\n",
    "\n",
    "    try:\n",
    "        rel_s = (lc[:,0]-t0_ijd)*24*3600\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    m = rel_s>-tstart_rel_mseconds\n",
    "    m &= rel_s<tstop_rel_seconds\n",
    "\n",
    "    print(\"total lc\",lc.shape)\n",
    "    print(\"min\",lc[:,0].min()-t0_ijd)\n",
    "    print(\"max\",lc[:,0].max()-t0_ijd)\n",
    "\n",
    "    lc = lc[m]\n",
    "\n",
    "    b_tb = np.mean(lc[:,1])    \n",
    "\n",
    "    rel_s = (lc[:,0]-t0_ijd)*24*3600\n",
    "\n",
    "    expected_telapse = tstop_rel_seconds + tstop_rel_seconds    \n",
    "\n",
    "    if len(rel_s) == 0:\n",
    "        telapse = 0\n",
    "        ontime = 0\n",
    "    else:\n",
    "        telapse = rel_s.max() - rel_s.min()\n",
    "        ontime = np.sum(lc[:,1])\n",
    "\n",
    "\n",
    "    print(\"expected telapse\", expected_telapse, \"telapse\", telapse, \"ontime\", ontime)\n",
    "\n",
    "    if float(ontime) / expected_telapse < required_completeness:\n",
    "        raise Exception(\"data not available: exected %.5lg elapsed %.5lg ontime %.5lg completeness %s requireed %s\"%(\n",
    "            expected_telapse, telapse, ontime,\n",
    "            ontime / expected_telapse, required_completeness))\n",
    "\n",
    "    lc_summary = dict()\n",
    "    summary[n.replace(\"/\",\"_\")]=lc_summary\n",
    "\n",
    "    print(\"size\", lc.shape, rel_s.shape)\n",
    "\n",
    "    if np.sum(m) == 0: continue\n",
    "\n",
    "    pylab.figure(figsize=(8,6))\n",
    "\n",
    "\n",
    "    for ascale in [0.05, 0.5, 1, 10]:\n",
    "        summary_scale = dict()\n",
    "        lc_summary[('s_%.5lg'%ascale).replace(\".\",\"_\")] = summary_scale        \n",
    "\n",
    "\n",
    "        print(\"requested scale\",ascale)\n",
    "        print(\"b_tb\",b_tb)\n",
    "\n",
    "        if b_tb>ascale:\n",
    "            ascale = b_tb\n",
    "\n",
    "\n",
    "        nscale = int(ascale/b_tb)\n",
    "        scale=nscale*b_tb\n",
    "\n",
    "        print(\"acceptable, will be\", nscale, scale)\n",
    "\n",
    "        rate = rebin(lc[:,2],nscale,False)/scale\n",
    "        rate_err = rebin(lc[:,2],nscale,False)**0.5/scale\n",
    "\n",
    "        print(\"rebinned to\",rate.shape)\n",
    "\n",
    "        pylab.errorbar(\n",
    "            rebin(rel_s,nscale,True),\n",
    "            rate,\n",
    "            rate_err,\n",
    "            xerr=scale/4.\n",
    "        )\n",
    "\n",
    "        summary_scale['meanrate'] = np.mean(rate)\n",
    "        summary_scale['maxrate'] = np.max(rate)\n",
    "        summary_scale['stdvar'] = np.std(rate)\n",
    "        summary_scale['meanerr'] = np.mean(rate_err**2)**0.5\n",
    "        summary_scale['excvar'] = summary_scale['stdvar']/summary_scale['meanerr']        \n",
    "\n",
    "        summary_scale['maxsnr'] = np.max((rate-np.mean(rate))/rate_err/summary_scale['excvar'])\n",
    "\n",
    "        summary_scale['localfar'] = stats.norm.sf(summary_scale['maxsnr'])*rate.shape[0]\n",
    "\n",
    "        summary_scale['localfar_s'] = stats.norm.isf(summary_scale['localfar']/2.) if summary_scale['localfar']<1 else 0\n",
    "\n",
    "        # add FAR spike here\n",
    "\n",
    "        if 'best' not in lc_summary or summary_scale['localfar_s'] > lc_summary['best']['localfar_s']:\n",
    "            lc_summary['best'] = dict(\n",
    "                localfar_s = summary_scale['localfar_s'],\n",
    "                scale = ascale,\n",
    "            )\n",
    "\n",
    "        print(summary_scale)\n",
    "\n",
    "\n",
    "    #tight_layout()\n",
    "    pylab.grid()\n",
    "\n",
    "    pylab.xlim(-tstart_rel_mseconds, tstop_rel_seconds)\n",
    "    #pylab.axhspan(0,10,alpha=0.2,color=\"red\")\n",
    "    #pylab.axhspan(10,15,alpha=0.2,color=\"green\")\n",
    "    #pylab.axhspan(15,20,alpha=0.2,color=\"blue\")\n",
    "    pylab.ylabel(n+\", count s$^{-1}$\")\n",
    "    #ylim([0,50])\n",
    "    pylab.xlabel(\"seconds since %s (IJD %.10lg)\"%(t0_utc, t0_ijd))\n",
    "\n",
    "    fn=n.replace(\"/\",\"_\") + \"_lc.png\"\n",
    "    pylab.savefig(fn)\n",
    "    print(\"saving as\",fn)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# below S/N of 4 FAR is determined primarily by poisson, above - by spikes\n",
    "\n",
    "def approx_FAR_spike_hz(snr, scale):    \n",
    "    lim_snr = 2\n",
    "    \n",
    "    spike_rate_snr6 = 60./3600./24.\n",
    "    if scale>=0.1:\n",
    "        spike_rate_snr6*=(scale/0.1)**-1\n",
    "    \n",
    "    \n",
    "    approx_FAR_hz = snr*0 + spike_rate_snr6 * (lim_snr/6.)**-2.7 \n",
    "    \n",
    "    try:\n",
    "        if snr>lim_snr:\n",
    "            approx_FAR_hz = spike_rate_snr6 * (np.abs(snr)/6.)**-2.7\n",
    "    except:\n",
    "        m=snr>lim_snr\n",
    "        approx_FAR_hz[m] = (np.abs(snr[m])/6.)**-2.7 * spike_rate_snr6\n",
    "        \n",
    "\n",
    "    return approx_FAR_hz\n",
    "\n",
    "def approx_FAR_norm_hz(snr, scale_s):\n",
    "    return stats.norm.sf(snr)/scale_s\n",
    "\n",
    "def approx_FAP(snr, t, scale_s):\n",
    "    \n",
    "    try:\n",
    "        t_scaled = t[:]\n",
    "        t_scaled[abs(t)<scale_s]=scale_s\n",
    "    except:\n",
    "        if abs(t)<scale_s:\n",
    "            t_scaled=scale_s\n",
    "        else:\n",
    "            t_scaled=t\n",
    "\n",
    "    approx_FAP = 2 * ( approx_FAR_norm_hz(snr, scale_s) + approx_FAR_spike_hz(snr, scale_s) )  * abs(t_scaled) * (1+np.log( 30/0.1))\n",
    "    \n",
    "    return approx_FAP\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pylab.figure()\n",
    "\n",
    "x=np.linspace(-5,10,100)\n",
    "\n",
    "for scale_s in 0.05, 0.1, 1, 10:\n",
    "\n",
    "    c=pylab.plot(x,approx_FAR_norm_hz(x, scale_s), ls='--')\n",
    "    pylab.plot(x,approx_FAR_spike_hz(x, scale_s),c=c[0].get_color(),ls=\":\")\n",
    "    pylab.plot(x,\n",
    "               approx_FAR_spike_hz(x, scale_s) + approx_FAR_norm_hz(x, scale_s),\n",
    "               c=c[0].get_color()\n",
    "              )\n",
    "\n",
    "\n",
    "    pylab.semilogy()\n",
    "\n",
    "pylab.ylim([1e-5, 30])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timescales = sorted(set([0.05*ns for ns in sorted(set(\n",
    "    list(map(int,np.logspace(0,np.log10(20*30),100))) \n",
    "))]  + list(np.linspace(1,31,30*2+1))))\n",
    "timescales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = dict()    \n",
    "all_excesses=[]            \n",
    "\n",
    "best_lc=None\n",
    "\n",
    "for n, lc in lcs.items():\n",
    "    \n",
    "    #rel_s = lc[:,0]\n",
    "    rel_s = (lc[:,0]-t0_ijd)*24*3600\n",
    "    \n",
    "\n",
    "    m = rel_s>-tstart_rel_mseconds\n",
    "    m &= rel_s<tstop_rel_seconds\n",
    "    \n",
    "    print(\"total lc\",lc.shape)\n",
    "    print(\"min\",lc[:,0].min()-t0_ijd)\n",
    "    print(\"max\",lc[:,0].max()-t0_ijd)\n",
    "    \n",
    "    lc = lc[m]\n",
    "   # rel_s = lc[:,0]\n",
    "    \n",
    "    b_tb = np.mean(lc[:,1])    \n",
    "    \n",
    "    rel_s = (lc[:,0]-t0_ijd)*24*3600\n",
    "    \n",
    "    expected_telapse = tstop_rel_seconds + tstop_rel_seconds    \n",
    "    \n",
    "    if len(rel_s) == 0:\n",
    "        telapse = 0\n",
    "        ontime = 0\n",
    "    else:\n",
    "        telapse = rel_s.max() - rel_s.min()\n",
    "        ontime = np.sum(lc[:,1])\n",
    "        \n",
    "    \n",
    "    print(\"expected telapse\", expected_telapse, \"telapse\", telapse, \"ontime\", ontime)\n",
    "    \n",
    "    if ontime / expected_telapse < required_completeness:\n",
    "        raise Exception(\"data not available: exected %.5lg elapsed %.5lg ontime %.5lg\"%(expected_telapse, telapse, ontime))\n",
    "        \n",
    "    lc_summary = dict()\n",
    "    summary[n.replace(\"/\",\"_\")]=lc_summary\n",
    "    \n",
    "    print(\"size\", lc.shape, rel_s.shape)\n",
    "    \n",
    "    if np.sum(m) == 0: continue\n",
    "    \n",
    "    pylab.figure(figsize=(8,6))\n",
    "    \n",
    "    best_lc_byscale={}\n",
    "\n",
    "        \n",
    "    \n",
    "    #for ascale in [0.05, 0.1, 0.2, 0.5, 1, 2, 10]:\n",
    "    for ascale in timescales:\n",
    "    # for ascale in [0.05, 0.1, 0.5]:\n",
    "    #for ascale in [0.05*i for i in range(20)] + [0.5*i for i in range(20)] + [15, 20, 25, 30]:\n",
    "    #for ascale in [0.05, 0.1, 0.15, 0.2, 0.25, 0.5, 1, 2, 8, 10]:\n",
    "    #for ascale in [1,]:\n",
    "        s_scale_mo = {}\n",
    "        lc_summary[('s_%.5lg'%ascale).replace(\".\",\"_\")] = s_scale_mo    \n",
    "        \n",
    "        \n",
    "        print(\"requested scale\",ascale)\n",
    "#        print(\"b_tb\",b_tb)\n",
    "        \n",
    "        if b_tb>ascale:\n",
    "            ascale = b_tb\n",
    "                    \n",
    "        \n",
    "        nscale = int(round(ascale/b_tb))\n",
    "        scale=nscale*b_tb\n",
    "\n",
    "        print(\"true scale\", scale)\n",
    "        \n",
    "#        print(\"acceptable, will be\", nscale, scale)\n",
    "        \n",
    "        c=None\n",
    "        \n",
    "        \n",
    "        #for offset in range(0,nscale):            \n",
    "        #for offset in (, round(nscale/2)):            \n",
    "        \n",
    "        if nscale < 20:\n",
    "            offsets = range(0, round(nscale/2)+1)\n",
    "        else:\n",
    "            offsets = range(0, round(nscale/2)+1, max(round(round(nscale/2)/20), 1))\n",
    "        \n",
    "        for offset in offsets: \n",
    "            summary_scale = dict()\n",
    "            s_scale_mo[offset]=summary_scale\n",
    "            \n",
    "            rel_s_scale = rebin(rel_s[offset:], nscale, True)\n",
    "            rate = rebin(lc[offset:,2],nscale,False)/scale        \n",
    "            rate_err = rebin(lc[offset:,2],nscale,False)**0.5/scale\n",
    "\n",
    "            #print(\"rebinned to\",rate.shape)\n",
    "            print(\"offset\", offset, \"rebinned to\",rate.shape)\n",
    "            \n",
    "            \n",
    "            summary_scale['scale_s']=scale\n",
    "            summary_scale['meanrate'] = np.mean(rate)\n",
    "            summary_scale['maxrate'] = np.max(rate)            \n",
    "            summary_scale['stdvar'] = np.std(rate)\n",
    "            summary_scale['meanerr'] = np.mean(rate_err**2)**0.5\n",
    "            summary_scale['excvar'] = summary_scale['stdvar']/summary_scale['meanerr']        \n",
    "\n",
    "            print(\"summary_scale['excvar']\", summary_scale['excvar'])\n",
    "\n",
    "            if negative_excesses==1:\n",
    "                snr = -(rate-np.mean(rate))/rate_err/summary_scale['excvar']\n",
    "            else:\n",
    "                snr = (rate-np.mean(rate))/rate_err/summary_scale['excvar']\n",
    "            \n",
    "            i_max = np.argmax(snr)\n",
    "            \n",
    "            print(i_max,snr[i_max],rel_s_scale[i_max])\n",
    "            \n",
    "            summary_scale['maxsnr'] = snr[i_max]\n",
    "            summary_scale['maxsnr_t'] = rel_s_scale[i_max]\n",
    "\n",
    "            summary_scale['localfar'] = stats.norm.sf(summary_scale['maxsnr'])*rate.shape[0]\n",
    "\n",
    "            summary_scale['localfar_s'] = stats.norm.isf(summary_scale['localfar']/2.) if summary_scale['localfar']<1 else 0\n",
    "            \n",
    "            m_over_threshold = snr > global_snr_threshold\n",
    "                        \n",
    "            excesses = dict(\n",
    "                            snr = snr[m_over_threshold],\n",
    "                            rel_s_scale = rel_s_scale[m_over_threshold],\n",
    "                            rate = rate[m_over_threshold],\n",
    "                            rate_err = rate_err[m_over_threshold],\n",
    "                            rate_overbkg = rate[m_over_threshold] - np.mean(rate),\n",
    "                        )\n",
    "                        \n",
    "            summary_scale['excesses'] = [dict(zip(excesses.keys(), er)) for er in zip(*excesses.values())]\n",
    "\n",
    "            for e in summary_scale['excesses']:\n",
    "                e['FAP'] = approx_FAP(e['snr'], e['rel_s_scale'], scale)\n",
    "            \n",
    "            all_excesses+=[\n",
    "                        dict(scale=scale, offset=offset,excess=e) for e in summary_scale['excesses']\n",
    "                    ]\n",
    "\n",
    "            print(\"scale\", scale, \"offset\", offset, \"found excesses\",len(summary_scale['excesses']))                        \n",
    "\n",
    "            #r=pylab.errorbar(\n",
    "            #    rebin(rel_s[offset:],nscale,True),\n",
    "            #    rate,\n",
    "            #    rate_err,\n",
    "            #    xerr=scale/4.,\n",
    "            #    c=c,\n",
    "            #    alpha=0.7\n",
    "            #)\n",
    "            \n",
    "        #    print(rel_s_scale.shape, snr.shape)\n",
    "            \n",
    "            r = pylab.errorbar(\n",
    "                rel_s_scale,\n",
    "                snr,\n",
    "                snr*0+1,\n",
    "                xerr=scale/4.,\n",
    "                c=c,\n",
    "                alpha=0.7\n",
    "            )\n",
    "            \n",
    "            pylab.axvline(summary_scale['maxsnr_t'],c=\"k\")\n",
    "            \n",
    "            \n",
    "            c=r[0].get_color()\n",
    "\n",
    "            \n",
    "            # add FAR spike here\n",
    "\n",
    "            if 'best' not in lc_summary or summary_scale['localfar_s'] > lc_summary['best']['localfar_s']:\n",
    "                lc_summary['best'] = dict(\n",
    "                    localfar_s = summary_scale['localfar_s'],\n",
    "                    scale = ascale,\n",
    "                    summary_scale = summary_scale,\n",
    "                )\n",
    "                best_lc=rel_s_scale,rate,rate_err\n",
    "            \n",
    "            if 'best' not in s_scale_mo or summary_scale['localfar_s'] > s_scale_mo['best']['localfar_s']:\n",
    "                s_scale_mo['best'] = dict(\n",
    "                    localfar_s = summary_scale['localfar_s'],\n",
    "                    scale = ascale,\n",
    "                    summary_scale = summary_scale,\n",
    "                )\n",
    "              #  best_lc=rel_s_scale,rate,rate_err\n",
    "            \n",
    "            if ascale not in best_lc_byscale or summary_scale['localfar_s'] > best_lc_byscale[ascale]['localfar_s']:\n",
    "                best_lc_byscale[ascale] = dict(\n",
    "                    localfar_s = summary_scale['localfar_s'],\n",
    "                    scale = ascale,\n",
    "                    summary_scale = summary_scale,\n",
    "                    best_lc=(rel_s_scale,rate,rate_err),\n",
    "                )\n",
    "                \n",
    "        \n",
    "            #print(summary_scale)\n",
    "        s_scale_mo.update(s_scale_mo['best']['summary_scale'])\n",
    "\n",
    "    #tight_layout()\n",
    "    pylab.grid()\n",
    "\n",
    "    #pylab.xlim(-tstart_rel_mseconds, tstop_rel_seconds)\n",
    "    #pylab.axhspan(0,10,alpha=0.2,color=\"red\")\n",
    "    #pylab.axhspan(10,15,alpha=0.2,color=\"green\")\n",
    "    #pylab.axhspan(15,20,alpha=0.2,color=\"blue\")\n",
    "    pylab.ylabel(n+\", S/N\")\n",
    "    #ylim([0,50])\n",
    "    pylab.xlabel(\"seconds since %s (IJD %.10lg)\"%(t0_utc, t0_ijd))\n",
    "    pylab.xlim([-10, 10])\n",
    "    \n",
    "    detfn=n.replace(\"/\",\"_\") + \"_det_lc.png\"\n",
    "    pylab.savefig(detfn)\n",
    "    print(\"saving as\",detfn)\n",
    "    \n",
    "summary['ACS']['best']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "\n",
    "snr = (rate-np.mean(rate))/rate_err/summary_scale['excvar']\n",
    "\n",
    "plt.step(\n",
    "    rel_s_scale,\n",
    "    rate\n",
    ")\n",
    "plt.xlim(-200,200)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in all_excesses:\n",
    "    if np.abs(e['scale'] - 0.05)< 0.001:\n",
    "        # if np.abs(e['excess']['rel_s_scale']) < 5:\n",
    "        print(e['excess']['rel_s_scale'], e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_excesses = []\n",
    "\n",
    "for i in sorted(all_excesses, key=lambda x:x['excess']['FAP']):\n",
    "    if i['excess']['FAP']<1 or True:\n",
    "        print(i['scale'],i['offset'], i['excess']['snr'], i['excess']['rel_s_scale'], i['excess']['FAP'])\n",
    "        \n",
    "        grouped=False\n",
    "        for g in grouped_excesses:\n",
    "            if abs(i['excess']['rel_s_scale']-g['excess']['rel_s_scale'])<max(i['scale'],g['scale']):\n",
    "                print(\"to group\", g['excess']['rel_s_scale'])\n",
    "                if i['excess']['snr'] > g['excess']['snr']:\n",
    "                    print(\"group takeover\")\n",
    "                    g.update(i)\n",
    "                grouped=True\n",
    "                \n",
    "        if not grouped:\n",
    "            print(\"new group\")\n",
    "            #i['group']=[i]\n",
    "            grouped_excesses.append(i)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_excesses=sorted(grouped_excesses, key=lambda x:x['excess']['FAP'])\n",
    "\n",
    "for i in grouped_excesses:\n",
    "    print(f\"timescale {i['scale']:4.2f}   S/N {i['excess']['snr']:5.2f}   T0+{i['excess']['rel_s_scale']:7.1f}   FAP {i['excess']['FAP']:7.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "len(json.dumps(grouped_excesses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary['ACS']['best']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary['ACS']['s_8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excvar_summary=dict(\n",
    ")\n",
    "\n",
    "for k,s in summary['ACS'].items():\n",
    "    if 'scale_s' in s:\n",
    "        print(\"%.5lg\"%s['scale_s'], \"%5.4lg\"%s['excvar'])\n",
    "        \n",
    "        if s['scale_s']<=0.200:\n",
    "            kg='hf_200ms'\n",
    "        elif s['scale_s']<=2.00:\n",
    "            kg='mf_200ms_2s'\n",
    "        elif s['scale_s']<=10.00:\n",
    "            kg='mf_2s_10s'\n",
    "        else:\n",
    "            kg='lf_10s'\n",
    "        \n",
    "        if kg not in excvar_summary:\n",
    "            excvar_summary[kg]=[s['excvar']]\n",
    "        else:\n",
    "            excvar_summary[kg]+=[s['excvar']]\n",
    "\n",
    "for k,v in excvar_summary.items():\n",
    "    print(k,min(v),max(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_names=[]\n",
    "\n",
    "\n",
    "for limit_group in 0.02, 0.1, 1: \n",
    "    figs=dict()\n",
    "\n",
    "    for n, lc in lcs.items():\n",
    "        rel_s = (lc[:,0]-t0_ijd)*24*3600\n",
    "\n",
    "        m = rel_s>-tstart_rel_mseconds\n",
    "        m &= rel_s<tstop_rel_seconds\n",
    "\n",
    "        print(\"total lc\",lc.shape)\n",
    "        print(\"min\",lc[:,0].min()-t0_ijd)\n",
    "        print(\"max\",lc[:,0].max()-t0_ijd)\n",
    "\n",
    "        lc = lc[m]\n",
    "        rel_s = rel_s[m]\n",
    "\n",
    "        for excess in grouped_excesses:\n",
    "            #if excess['excess']['FAP'] > 0.02: continue\n",
    "            if excess['excess']['FAP'] > limit_group: continue\n",
    "\n",
    "            print(excess)\n",
    "\n",
    "\n",
    "            offset = excess['offset']\n",
    "            nscale = int(excess['scale']/b_tb)  \n",
    "            scale=excess['scale']\n",
    "\n",
    "            s_figs = sorted(figs.items(), key=lambda x:abs(x[0]-scale))\n",
    "\n",
    "            if len(s_figs) == 0 or s_figs[0][0] < scale*0.5 or s_figs[0][0] > scale*1.5: \n",
    "                fig = pylab.figure(figsize=(8,6))\n",
    "                figs[scale] = fig\n",
    "                pylab.xlim([-2,2])\n",
    "                pylab.xlabel(\"seconds since \"+t0_utc)\n",
    "                pylab.ylabel(\"counts/s\")\n",
    "                pylab.title(\"FAP threshold %.5lg\"%limit_group)\n",
    "            else:            \n",
    "                print(\"good match\", s_figs[0][0], scale)\n",
    "                pylab.figure(s_figs[0][1].number)\n",
    "                pylab.xlabel(\"seconds since \"+t0_utc)\n",
    "                pylab.ylabel(\"counts/s\")\n",
    "\n",
    "\n",
    "\n",
    "            rel_s_scale = rebin(rel_s[offset:],nscale,True)\n",
    "            rate = rebin(lc[offset:,2],nscale,False)/scale        \n",
    "            rate_err = rebin(lc[offset:,2],nscale,False)**0.5/scale\n",
    "\n",
    "            bkg=np.mean(rate)\n",
    "\n",
    "            m_on = np.abs(rel_s_scale-excess['excess']['rel_s_scale'])<excess['scale']*1.5\n",
    "\n",
    "\n",
    "            pylab.grid(False)\n",
    "\n",
    "            pylab.axhline(0, alpha=0.2, ls=\":\", color='gray')\n",
    "\n",
    "            cr=pylab.errorbar(\n",
    "                    rel_s_scale, \n",
    "                    (rate-bkg), \n",
    "                    (rate_err),\n",
    "                    alpha=0.5,\n",
    "                    ls=\"\",\n",
    "                )[0].get_color()\n",
    "\n",
    "            pylab.step(\n",
    "                    rel_s_scale, \n",
    "                    (rate-bkg), \n",
    "              #      (rate_err),\n",
    "                    alpha=0.5,\n",
    "                    where=\"mid\",\n",
    "                    c=cr,\n",
    "                )\n",
    "\n",
    "            pylab.axhline(np.std(rate)*3, alpha=0.2, ls=\"--\",c=cr)\n",
    "            pylab.axhline(np.std(rate)*5, alpha=0.2, ls=\"--\", lw=2,c=cr)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            pylab.errorbar(\n",
    "                    rel_s_scale[m_on], \n",
    "                    (rate-bkg)[m_on], \n",
    "                    (rate_err)[m_on],\n",
    "                    lw=2.,\n",
    "                    alpha=1,\n",
    "                    label=\"S/N %.3lg FAP %.3lg scale %.3lg s\"%(excess['excess']['snr'],excess['excess']['FAP'],excess['scale']),\n",
    "                    c=cr\n",
    "                )\n",
    "\n",
    "\n",
    "\n",
    "            newlim=([\n",
    "                min([excess['excess']['rel_s_scale']*1.3-excess['scale']*5,-excess['scale']*5]),\n",
    "                max([excess['excess']['rel_s_scale']*1.3+excess['scale']*5,excess['scale']*5]),\n",
    "            ])\n",
    "\n",
    "            oldlim=pylab.gca().get_xlim()\n",
    "\n",
    "            print(oldlim)\n",
    "\n",
    "            pylab.xlim([\n",
    "                min([oldlim[0],newlim[0]]),\n",
    "                max([oldlim[1],newlim[1]]),\n",
    "            ])\n",
    "\n",
    "\n",
    "    for f_i,(s,f) in enumerate(figs.items()):\n",
    "        f.legend()\n",
    "        f.gca().axvline(0,ls=\"--\",c=\"r\",lw=3)\n",
    "        fn=\"excess_%.5lg_%i.png\"%(s,len(fig_names))\n",
    "        f.savefig(fn)\n",
    "        fig_names.append(fn)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=1\n",
    "rows=int(np.ceil(len(fig_names)/cols))\n",
    "\n",
    "\n",
    "if rows>0:\n",
    "    f, axes=pylab.subplots(rows, cols, figsize=(12, 8*rows))\n",
    "    print(\"axes\",axes,axes.__class__)\n",
    "\n",
    "    if rows>1:\n",
    "        axes=axes.flatten()\n",
    "    else:\n",
    "        axes=[axes]\n",
    "\n",
    "    for i,fn in enumerate(fig_names):\n",
    "        #f.add_subplot(len(fig_names), 2, i+1)\n",
    "        axes[i].axis('off')\n",
    "        axes[i].imshow(pylab.imread(fn) ) #, extent=(0,1,0,1))\n",
    "        #pylab.imshow(pylab.imread(fn), extent=(0,1,(i-1)/len(fig_names),i/len(fig_names)))\n",
    "\n",
    "    f.tight_layout()\n",
    "else:\n",
    "    f=pylab.figure()\n",
    "\n",
    "f.savefig(\"excesses_mosaic.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if rt == 1:    \n",
    "    summary['ACS_rt'] = summary['ACS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary['ACS']['s_1']['meanerr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json.dump(dict(\n",
    "    summary=summary,\n",
    "    reportable_excesses=grouped_excesses,\n",
    "    excvar_summary=excvar_summary\n",
    "), open(\"integral_all_sky.json\",\"w\"), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputs"
    ]
   },
   "outputs": [],
   "source": [
    "acs_lc_png=\"ACS_lc.png\"\n",
    "acs_rt_lc_png=\"ACS_lc.png\"\n",
    "acs_rt_det_lc_png=\"ACS_det_lc.png\"\n",
    "ibis_veto_lc_png=\"IBIS_Veto_lc.png\"\n",
    "excesses_mosaic_png=\"excesses_mosaic.png\"\n",
    "summary=summary\n",
    "reportable_excesses=grouped_excesses\n",
    "excvar_summary=excvar_summary"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
